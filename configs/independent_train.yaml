# General Config Parameters
model_name: HuggingFaceTB/SmolLM2-135M            # Hugginface name of the model
run_name: "stage0"                  # Unique run name for this experiment
run_dir: ""                                       # Directory name (creates it does not exist) 
seed: 42                                          # Seed for reproducing the results

# Data related settings
data:
  dataset_name: roneneldan/TinyStories            # Huggingface dataset name
  split: train                      
  max_length: 1024
  column_name: 'text'
  streaming: true
  dataset_size: 49188244

# Training related settings
training:
  pretrain_from_scratch: true                     # Whether to pretrain the model from scratch
  init_method: 'kaiming_normal'                   # ['xavier_uniform', 'kaiming_normal', 'normal']
  lr: 4e-3
  weight_decay: 0.01
  eps: 1e-8
  beta1: 0.9
  beta2: 0.98
  batch_size: 42
  gradient_accumulation_steps: 10
  epochs: 1
  bf16: true
  fp16: false 
  ddp_find_unused_parameters: false
  max_grad_norm: 1.0
  dataloader_drop_last: true

# Lr scheduler settings
scheduler:
  name: 'constant_with_warmup'                    # 'warmup_stable_decay'
  warmup_ratio: 0.0
  num_cycles: 0.5
  decay_ratio: 1.0
  warmup_type: 'linear'
  decay_type: 'cosine'
  min_lr_ratio: 0.0

# Logging related parameters
logging:
  overwrite_output_dir: true
  save_steps_ratio: 0.25
  save_total_limit: 5
  logging_steps: 100
  report_to: 'wandb'
  save_strategy: 'epoch'
  logging_strategy: 'steps'

